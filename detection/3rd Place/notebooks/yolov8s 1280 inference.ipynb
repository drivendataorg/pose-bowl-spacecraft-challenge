{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac2a7dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import openvino\n",
    "import pandas as pd\n",
    "from loguru import logger\n",
    "from tqdm import tqdm\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d3a365a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"/home/ashish/solution\"  # replace with the test dataset directory\n",
    "OUTPUT_PATH = \"/home/ashish/solution/submission.csv\"  # replace where submission file needs to be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23d615c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-05-20 19:11:13.540\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m47\u001b[0m - \u001b[1musing data dir: /home/ashish/drivendata/solution/fold0\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024.1.0-15008-f4afc983258-releases/2024/1\n",
      "Ultralytics YOLOv8.1.42 üöÄ Python-3.10.12 torch-2.1.2 CPU (AMD Ryzen Threadripper 3960X 24-Core Processor)\n",
      "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'assets/model.pt' with input shape (1, 3, 1280, 1280) BCHW and output shape(s) (1, 5, 33600) (64.1 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mOpenVINO:\u001b[0m starting export with openvino 2024.1.0-15008-f4afc983258-releases/2024/1...\n",
      "\u001b[34m\u001b[1mOpenVINO:\u001b[0m export success ‚úÖ 2.0s, saved as 'assets/model_openvino_model/' (43.0 MB)\n",
      "\n",
      "Export complete (3.9s)\n",
      "Results saved to \u001b[1m/home/ashish/drivendata/solution/assets\u001b[0m\n",
      "Predict:         yolo predict task=detect model=assets/model_openvino_model imgsz=1280  \n",
      "Validate:        yolo val task=detect model=assets/model_openvino_model imgsz=1280 data=/home/ashish/drivendata/yolov8x640/fold0/gbr.yaml  \n",
      "Visualize:       https://netron.app\n",
      "WARNING ‚ö†Ô∏è Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-05-20 19:11:17.570\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1m  0%|          | 0/1089 [00:00<?, ?it/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading assets/model_openvino_model for OpenVINO inference...\n",
      "Using OpenVINO LATENCY mode for batch=1 inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-05-20 19:11:30.786\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1m  9%|‚ñâ         | 100/1089 [00:13<02:10,  7.57it/s]\u001b[0m\n",
      "\u001b[32m2024-05-20 19:11:41.389\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1m 18%|‚ñà‚ñä        | 200/1089 [00:23<01:43,  8.56it/s]\u001b[0m\n",
      "\u001b[32m2024-05-20 19:11:51.845\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1m 28%|‚ñà‚ñà‚ñä       | 300/1089 [00:34<01:27,  8.99it/s]\u001b[0m\n",
      "\u001b[32m2024-05-20 19:12:02.619\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1m 37%|‚ñà‚ñà‚ñà‚ñã      | 400/1089 [00:45<01:15,  9.10it/s]\u001b[0m\n",
      "\u001b[32m2024-05-20 19:12:14.017\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1m 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 500/1089 [00:56<01:05,  8.98it/s]\u001b[0m\n",
      "\u001b[32m2024-05-20 19:12:25.204\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1m 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 600/1089 [01:07<00:54,  8.97it/s]\u001b[0m\n",
      "\u001b[32m2024-05-20 19:12:36.354\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1m 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 700/1089 [01:18<00:43,  8.97it/s]\u001b[0m\n",
      "\u001b[32m2024-05-20 19:12:47.378\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1m 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 800/1089 [01:29<00:32,  9.00it/s]\u001b[0m\n",
      "\u001b[32m2024-05-20 19:12:58.326\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1m 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 900/1089 [01:40<00:20,  9.05it/s]\u001b[0m\n",
      "\u001b[32m2024-05-20 19:13:09.733\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1m 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 1000/1089 [01:52<00:10,  8.09it/s]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(openvino.__version__)\n",
    "\n",
    "\n",
    "def centered_box(img, scale=0.1):\n",
    "    \"\"\"\n",
    "    Return coordinates for a centered bounding box on the image, defaulting to 10% of the image's height and width.\n",
    "    \"\"\"\n",
    "    # Get image dimensions\n",
    "    height, width, _ = img.shape\n",
    "    # Calculate the center of the image\n",
    "    center_x, center_y = width // 2, height // 2\n",
    "    # Calculate 10% of the image's height and width for the bounding box\n",
    "    box_width, box_height = width * scale, height * scale\n",
    "    # Calculate top-left corner of the bounding box\n",
    "    x1 = center_x - box_width // 2\n",
    "    y1 = center_y - box_height // 2\n",
    "    # Calculate bottom-right corner of the bounding box\n",
    "    x2 = center_x + box_width // 2\n",
    "    y2 = center_y + box_height // 2\n",
    "\n",
    "    return [x1, y1, x2, y2]\n",
    "\n",
    "\n",
    "def main(data_dir, output_path):\n",
    "    # locate key files and locations\n",
    "    data_dir = Path(data_dir).resolve()\n",
    "    output_path = Path(output_path).resolve()\n",
    "    submission_format_path = data_dir / \"submission_format.csv\"\n",
    "    images_dir = data_dir / \"images\"\n",
    "\n",
    "    assert data_dir.exists(), f\"Data directory does not exist: {data_dir}\"\n",
    "    assert output_path.parent.exists(), f\"Expected output directory {output_path.parent} does not exist\"\n",
    "    assert submission_format_path.exists(), f\"Expected submission format file {submission_format_path} does not exist\"\n",
    "    assert images_dir.exists(), f\"Expected images dir {images_dir} does not exist\"\n",
    "    logger.info(f\"using data dir: {data_dir}\")\n",
    "\n",
    "    # copy the submission format file; we'll use this as template and overwrite placeholders with our own predictions\n",
    "    submission_format_df = pd.read_csv(submission_format_path, index_col=\"image_id\")\n",
    "    submission_df = submission_format_df.copy()\n",
    "    # load pretrained model we included in our submission.zip\n",
    "    model = YOLO(\"assets/model.pt\")\n",
    "    model.export(format=\"openvino\", imgsz=1280)\n",
    "    model = YOLO(\"assets/model_openvino_model\")\n",
    "    # add a progress bar using tqdm without spamming the log\n",
    "    update_iters = min(100, int(submission_format_df.shape[0] / 10))\n",
    "    with open(os.devnull, \"w\") as devnull:\n",
    "        progress_bar = tqdm(\n",
    "            enumerate(submission_format_df.index.values),\n",
    "            total=submission_format_df.shape[0],\n",
    "            miniters=update_iters,\n",
    "            file=devnull,\n",
    "        )\n",
    "        # generate predictions for each image\n",
    "        for i, image_id in progress_bar:\n",
    "            if (i % update_iters) == 0:\n",
    "                logger.info(str(progress_bar))\n",
    "            # load the image\n",
    "            img = cv2.imread(str(images_dir / f\"{image_id}.png\"))\n",
    "            # get yolo result\n",
    "            result = model(img, imgsz=1280, verbose=False, conf=0.0001)[0]\n",
    "            # get bbox coordinates if they exist, otherwise just get a generic box in center of an image\n",
    "            bbox = result.boxes.xyxy[0].tolist() if len(result.boxes) > 0 else centered_box(img)\n",
    "            # convert bbox values to integers\n",
    "            bbox = [int(x) for x in bbox]\n",
    "            # store the result\n",
    "            submission_df.loc[image_id] = bbox\n",
    "\n",
    "    # write the submission to the submission output path\n",
    "    submission_df.to_csv(output_path, index=True)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(DATA_DIR, OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
