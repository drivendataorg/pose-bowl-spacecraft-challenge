{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc8e18df",
   "metadata": {},
   "source": [
    "# This notebook shows the training pipeline for Pose Bowl: Detection Track competition to train yolov8s model.Following are few points to consider:\n",
    "- replace the DATA_DIR folder with your own data directory where data is stored\n",
    "- replace the ROOT_DIR folder with your current working directory. In case both are same then ROOT_DIR = DATA_DIR\n",
    "- during training each epoch checkpoint will be saved to ROOT_DIR + 'fold0/train' and epoch28 checkpoint will be used for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0753288",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ashish/anaconda3/envs/pytorch/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import pandas as pd\n",
    "import os, cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import glob, shutil\n",
    "from joblib import Parallel, delayed\n",
    "import torch\n",
    "import yaml, gc\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold\n",
    "from utils import coco2yolo, coco2voc, voc2yolo\n",
    "from utils import draw_bboxes, load_image\n",
    "from utils import clip_bbox, str2annot, annot2str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8715d454",
   "metadata": {},
   "source": [
    "# Create Labels\n",
    "We need to export our labels to **YOLO** format, with one `*.txt` file per image (if no objects in image, no `*.txt` file is required). The *.txt file specifications are:\n",
    "\n",
    "* One row per object\n",
    "* Each row is class `[x_center, y_center, width, height]` format.\n",
    "* Box coordinates must be in **normalized** `xywh` format (from `0 - 1`). If your boxes are in pixels, divide `x_center` and `width` by `image width`, and `y_center` and `height` by `image height`.\n",
    "* Class numbers are **zero-indexed** (start from `0`).\n",
    "\n",
    "> Competition bbox format is **pascal voc** hence `[x_min, y_min, width, height]`. So, we need to convert form **pascal voc** to **YOLO** format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31d0a256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code \n",
    "def create_label_file(df):\n",
    "    cnt = 0\n",
    "    all_bboxes = []\n",
    "    bboxes_info = []\n",
    "    for row_idx in tqdm(range(df.shape[0])):\n",
    "        row = df.iloc[row_idx]\n",
    "        image_height = row.img_height\n",
    "        image_width  = row.img_width\n",
    "        bboxes_voc  = np.array([[row.xmin, row.ymin, row.xmax, row.ymax]]).astype(np.float32).copy()\n",
    "        num_bbox     = len(bboxes_voc)\n",
    "        names        = ['satellite']*num_bbox\n",
    "        labels       = np.array([0]*num_bbox)[..., None].astype(str)\n",
    "        ## Create Annotation(YOLO)\n",
    "        with open(row.label_path, 'w') as f:\n",
    "            if num_bbox<1:\n",
    "                annot = ''\n",
    "                f.write(annot)\n",
    "                cnt+=1\n",
    "                continue\n",
    "            bboxes_voc  = clip_bbox(bboxes_voc, image_height, image_width)\n",
    "            bboxes_yolo = voc2yolo(bboxes_voc, image_height, image_width).astype(str)\n",
    "            all_bboxes.extend(bboxes_yolo.astype(float))\n",
    "            bboxes_info.extend([[row.image_id]]*len(bboxes_yolo))\n",
    "            annots = np.concatenate([labels, bboxes_yolo], axis=1)\n",
    "            string = annot2str(annots)\n",
    "            f.write(string)\n",
    "    print('Missing:',cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56144533",
   "metadata": {},
   "source": [
    "## # Create Folds\n",
    "> The following code split spacecraft_id and background_id into 25 folds each containing unique ids that do not intersect with other folds.I have used only fold 0 for validation and it perfectly coorelates with the public leaderboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4524543",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_validation_set(df):\n",
    "    kf = GroupKFold(n_splits = 5)\n",
    "    df['kfold'] = -1\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(df['image_id'], y = df['spacecraft_id'], groups = df['background_id'])):   \n",
    "        df.loc[val_idx, 'kfold'] = fold\n",
    "\n",
    "    j = -1\n",
    "    for i, ids in enumerate(np.array_split(df['spacecraft_id'].unique(), indices_or_sections = 5)):\n",
    "        for k in range(5):\n",
    "            j += 1\n",
    "            df.loc[(df['spacecraft_id'].isin(ids)) & (df['background_id'].isin(df[df['kfold'] == k].background_id.unique())), 'fold'] = j\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85689db9",
   "metadata": {},
   "source": [
    "#  Write Images\n",
    "* We need to copy the Images to Current Directory as training data are split into multiple files.\n",
    "* We can make this process faster using **Joblib** which uses **Parallel** computing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d743d7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_copy(row):\n",
    "    shutil.copyfile(row.old_image_path, row.image_path)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0704fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create folder to save images and labels in current directory\n",
    "\n",
    "def create_folder(IMAGE_DIR, LABEL_DIR):\n",
    "    if os.path.exists(IMAGE_DIR):\n",
    "        !rm -rf {IMAGE_DIR}\n",
    "        !rm -rf {LABEL_DIR}\n",
    "    !mkdir -p {IMAGE_DIR}\n",
    "    !mkdir -p {LABEL_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb9549c",
   "metadata": {},
   "source": [
    "#  Configuration\n",
    "The dataset config file requires\n",
    "1. The dataset root directory path and relative paths to `train / val / test` image directories (or *.txt files with image paths)\n",
    "2. The number of classes `nc` and \n",
    "3. A list of class `names`:`['sat']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d850cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_yaml_file(train_df, valid_df, cwd,fold):\n",
    "    with open(os.path.join( cwd , 'train.txt'), 'w') as f:\n",
    "        for path in train_df.image_path.tolist():\n",
    "            f.write(path+'\\n')\n",
    "\n",
    "    with open(os.path.join(cwd , 'val.txt'), 'w') as f:\n",
    "        for path in valid_df.image_path.tolist():\n",
    "            f.write(path+'\\n')\n",
    "\n",
    "    data = dict(\n",
    "        train =  os.path.join( cwd , 'train.txt'),\n",
    "        val   =  os.path.join( cwd , 'val.txt' ),\n",
    "        nc    = 1,\n",
    "        names = ['sat'],\n",
    "        )\n",
    "\n",
    "    with open(os.path.join( cwd , 'gbr.yaml'), 'w') as outfile:\n",
    "        yaml.dump(data, outfile, default_flow_style=False)\n",
    "        \n",
    "    f = open(os.path.join( cwd , 'gbr.yaml'), 'r')\n",
    "    print('\\nyaml:')\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f636f0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_valid_df(valid_df, fold):\n",
    "#     valid_df.reset_index(drop = True).to_csv('code_execution/data/submission_format.csv', index = False)\n",
    "    COLUMNS = ['image_id', 'xmin', 'ymin', 'xmax', 'ymax']\n",
    "    valid_df[COLUMNS].to_csv( ROOT_DIR + f'fold{fold}/fold0_true_df.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ab3fee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import yaml\n",
    "# with open('hyp.yml') as file:\n",
    "#     hyp = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8a8cdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2c53188",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '/home/ashish/satellite/'  # \n",
    "ROOT_DIR = '/home/ashish/solution/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f72d31",
   "metadata": {},
   "source": [
    "# training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3af54060",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25801/25801 [00:08<00:00, 3045.85it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25801/25801 [00:04<00:00, 5308.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing: 0\n",
      "fold :  0 train_images :  25801 val_images :  1089\n",
      "\n",
      "yaml:\n",
      "names:\n",
      "- sat\n",
      "nc: 1\n",
      "train: /home/ashish/drivendata/solution/fold0/train.txt\n",
      "val: /home/ashish/drivendata/solution/fold0/val.txt\n",
      "\n",
      "New https://pypi.org/project/ultralytics/8.2.18 available ðŸ˜ƒ Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.1.42 ðŸš€ Python-3.10.12 torch-2.1.2 CUDA:0 (NVIDIA RTX A6000, 48655MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=None, data=/home/ashish/drivendata/solution/fold0/gbr.yaml, epochs=30, time=None, patience=100, batch=32, imgsz=1280, save=True, save_period=1, cache=False, device=None, workers=8, project=/home/ashish/drivendata/solution/fold0, name=train, exist_ok=False, pretrained=True, optimizer=AdamW, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=5, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=False, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.001, lrf=0.01, momentum=0.9, weight_decay=0.0005, warmup_epochs=2.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=10.0, cls=0.5, dfl=6.0, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.0, hsv_s=0.0, hsv_v=0.0, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=0.25, mixup=0.25, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=hyp.yml, tracker=botsort.yaml, save_dir=/home/ashish/drivendata/solution/fold0/train\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
      " 22        [15, 18, 21]  1   2116435  ultralytics.nn.modules.head.Detect           [1, [128, 256, 512]]          \n",
      "Model summary: 225 layers, 11135987 parameters, 11135971 gradients, 28.6 GFLOPs\n",
      "\n",
      "Transferred 349/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/ashish/drivendata/solution/fold0/labels... 25801 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25801/25801 [01:52<00:00, 228.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /home/ashish/drivendata/solution/fold0/labels.cache\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.001, blur_limit=(3, 7)), MedianBlur(p=0.001, blur_limit=(3, 7)), ToGray(p=0.001), CLAHE(p=0.001, clip_limit=(1, 4.0), tile_grid_size=(8, 8)), RandomRotate90(p=0.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/ashish/drivendata/solution/fold0/labels... 1089 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1089/1089 [00:05<00:00, 191.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/ashish/drivendata/solution/fold0/labels.cache\n",
      "Plotting labels to /home/ashish/drivendata/solution/fold0/train/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 1280 train, 1280 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1m/home/ashish/drivendata/solution/fold0/train\u001b[0m\n",
      "Starting training for 30 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/807 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/30      28.5G      2.483       26.3      7.752         39       1280:   0%|          | 3/807 [00:04<18:28,  1.38s/it]\n",
      "Exception in thread Thread-81 (_pin_memory_loop):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ashish/anaconda3/envs/pytorch/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/ashish/anaconda3/envs/pytorch/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ashish/anaconda3/envs/pytorch/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py\", line 54, in _pin_memory_loop\n",
      "    do_one_step()\n",
      "  File \"/home/ashish/anaconda3/envs/pytorch/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py\", line 31, in do_one_step\n",
      "    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/home/ashish/anaconda3/envs/pytorch/lib/python3.10/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/ashish/anaconda3/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/reductions.py\", line 355, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/home/ashish/anaconda3/envs/pytorch/lib/python3.10/multiprocessing/resource_sharer.py\", line 57, in detach\n",
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "  File \"/home/ashish/anaconda3/envs/pytorch/lib/python3.10/multiprocessing/resource_sharer.py\", line 86, in get_connection\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "  File \"/home/ashish/anaconda3/envs/pytorch/lib/python3.10/multiprocessing/connection.py\", line 502, in Client\n",
      "    c = SocketClient(address)\n",
      "  File \"/home/ashish/anaconda3/envs/pytorch/lib/python3.10/multiprocessing/connection.py\", line 630, in SocketClient\n",
      "    s.connect(address)\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m get_yaml_file(train_df, valid_df, cwd,fold) \u001b[38;5;66;03m#get configuration file\u001b[39;00m\n\u001b[1;32m     34\u001b[0m model \u001b[38;5;241m=\u001b[39m YOLO(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myolov8s.pt\u001b[39m\u001b[38;5;124m'\u001b[39m, task \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdetect\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m#define and load pretrained yolov8s model\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgbr.yaml\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhyp.yml\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproject\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mROOT_DIR\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfold\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfold\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#start training\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.10/site-packages/ultralytics/engine/model.py:667\u001b[0m, in \u001b[0;36mModel.train\u001b[0;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[1;32m    664\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mhub_session \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession  \u001b[38;5;66;03m# attach optional HUB session\u001b[39;00m\n\u001b[0;32m--> 667\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m}:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.10/site-packages/ultralytics/engine/trainer.py:198\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    195\u001b[0m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 198\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.10/site-packages/ultralytics/engine/trainer.py:383\u001b[0m, in \u001b[0;36mBaseTrainer._do_train\u001b[0;34m(self, world_size)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;66;03m# Optimize - https://pytorch.org/docs/master/notes/amp_examples.html\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ni \u001b[38;5;241m-\u001b[39m last_opt_step \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccumulate:\n\u001b[0;32m--> 383\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m     last_opt_step \u001b[38;5;241m=\u001b[39m ni\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;66;03m# Timed stopping\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.10/site-packages/ultralytics/engine/trainer.py:547\u001b[0m, in \u001b[0;36mBaseTrainer.optimizer_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mema:\n\u001b[0;32m--> 547\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mema\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.10/site-packages/ultralytics/utils/torch_utils.py:456\u001b[0m, in \u001b[0;36mModelEMA.update\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m v\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mis_floating_point:  \u001b[38;5;66;03m# true for FP16 and FP32\u001b[39;00m\n\u001b[1;32m    455\u001b[0m     v \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m d\n\u001b[0;32m--> 456\u001b[0m     v \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m d) \u001b[38;5;241m*\u001b[39m \u001b[43mmsd\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    fold = 0\n",
    "    df = pd.read_csv(DATA_DIR + 'train_labels.csv')\n",
    "    metadata = pd.read_csv(DATA_DIR + 'train_metadata.csv')\n",
    "    df = pd.merge(df, metadata, on = 'image_id').reset_index(drop = True)\n",
    "    df['id'] = df.image_id.str[0]\n",
    "    df['old_image_path']  = DATA_DIR +df.image_id.str[0] + '/images/' + df.image_id + '.png'\n",
    "    df['img_height'] = 1024\n",
    "    df['img_width'] = 1280\n",
    "\n",
    "    df = create_validation_set(df) # create folds\n",
    "\n",
    "    IMAGE_DIR = ROOT_DIR + f'fold{fold}/images' # directory to save images\n",
    "    LABEL_DIR = ROOT_DIR + f'fold{fold}/labels' # directory to save labels\n",
    "    \n",
    "    df['image_path']  = f'{IMAGE_DIR}/' +  df.image_id + '.png' # image and label paths to read images and labels by yolov8\n",
    "    df['label_path']  = f'{LABEL_DIR}/' +  df.image_id + '.txt'\n",
    "    \n",
    "    valid_df = df[df['fold'] == fold]   #yolo needs both val and train txt files for training\n",
    "#     train_df = df[(~df['spacecraft_id'].isin(valid_df.spacecraft_id.unique())) & \n",
    "#                   (~df['background_id'].isin(valid_df.background_id.unique()))].reset_index(drop = True)   \n",
    "    train_df = df.copy()\n",
    "    create_folder(IMAGE_DIR, LABEL_DIR)\n",
    "#     save_valid_df(valid_df, fold)  used for validating the model on oof\n",
    "    \n",
    "    # copying training images from multiple directory into single directory\n",
    "    image_paths = df.old_image_path.tolist()\n",
    "    _ = Parallel(n_jobs=-1, backend='threading')(delayed(make_copy)(row) for _, row in tqdm(df.iterrows(), total=len(df)))   \n",
    "    create_label_file(df)\n",
    "    print('fold : ', fold, 'train_images : ', len(train_df),'val_images : ', len(valid_df))\n",
    "    cwd = ROOT_DIR + f'fold{fold}'\n",
    "    get_yaml_file(train_df, valid_df, cwd,fold) #get configuration file\n",
    "    \n",
    "    model = YOLO('yolov8s.pt', task = 'detect')  #define and load pretrained yolov8s model\n",
    "    results = model.train(data=os.path.join(cwd , 'gbr.yaml'),cfg = 'hyp.yml', project = ROOT_DIR + f'fold{fold}') #start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "228e91a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_valid_df(valid_df, fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9d8f15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
